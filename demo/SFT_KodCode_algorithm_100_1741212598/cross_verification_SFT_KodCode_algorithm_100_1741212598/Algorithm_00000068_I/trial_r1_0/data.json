{
  "style": "instruct",
  "subset": "Algorithm",
  "question_id": "Algorithm_00000068_I",
  "question": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "test_info": [
    {
      "function_name": "activation_function",
      "parameter_list": "(vector, mode)",
      "function_declaration": "def activation_function(vector, mode):",
      "docstring": "Returns the numpy array of the values after applying the specified activation function."
    }
  ],
  "gpt_pass_sequence": [
    0,
    0,
    1
  ],
  "gpt_pass_trial_num": 1,
  "gpt_difficulty": "medium",
  "gpt_pass_percentage": 0.333,
  "r1_response": "<think>\nAlright, so I need to implement this activation_function in Python. The function takes a numpy array and a mode, and applies the corresponding activation function. Hmm, okay. Let's think about what each activation function does.\n\nFirst, binary_step. Oh right, it's also called the Heaviside step function. It outputs 0 if the input is less than 0, else it outputs 1. So for each element in the vector, if it's negative, replace it with 0; else, 1. But wait, wait\u2014what about when it's exactly zero? Oh, the examples show that 0 is treated as 1. So, yes, anything >=0 becomes 1.\n\nThen there's ReLU. That's straightforward: for each element in the vector, if it's negative, replace with 0; otherwise, leave it as is. So ReLU is max(0, x) for each x.\n\nAnd then the sigmoid function. Sigmoid maps a real number to a value between 0 and 1. The formula is 1/(1 + e^(-x)), right? So each element x in the vector is transformed using this formula.\n\nSo the function needs to, based on the mode, apply one of these three functions.\n\nNow, how to implement this efficiently, especially for large vectors?\n\nI remember that numpy is optimized for vectorized operations, which are much faster than iterating element-wise in Python. So I should avoid loops and use numpy's functions wherever possible.\n\nFor each mode, I can handle it using vectorized operations.\n\nStarting with binary_step: For each element in the vector, output 1 if element >=0 else 0. So how can I compute this efficiently in numpy?\n\nI can use numpy's where function. Alternatively, another approach is to compute (vector >= 0).astype(int). Because when vector>=0 is True, it's 1, else 0. So the binary_step can be done with:\n\nbinary_output = (vector >= 0).astype(int)\n\nWait, let's test that logic with the sample input.\n\nSample for binary step:\n\nInput: [-1.2,0,2,1.45,-3.7,0.3]\n\nThe >=0 gives [False, True, True, True, False, True]. astype(int) converts to [0,1,1,1,0,1], which matches the sample output. So that's correct.\n\nNext, ReLU. For vector, ReLU is max(0, x) for each x. So we can compute the maximum of the vector and zero.\n\nrelu_output = np.maximum(vector, 0)\n\nYes, that's correct because for each element, if it's positive it remains, else becomes zero.\n\nWhat about the sample ReLU input? [-1,0,1,-2,2]. Applying max with 0 gives [0,0,1,0,2], which matches the sample.\n\nSigmoid is a bit more involved. I think the formula is sigmoid = 1 / (1 + np.exp(-x)). So applying that for each element.\n\nSo I compute exponentiated versions of the negative of each element x, add 1, take reciprocal.\n\nIn code, that's:\n\nsigmoid_output = 1 / (1 + np.exp(-vector))\n\nBut wait, since all operations are element-wise, it's automatically applied to the entire array. So that should work.\n\nLet me check the sample input for sigmoid: [-1,0,1]\n\nCalculating each:\n\n-1: exp(1) is e \u22482.718. So 1/(1 + e^1) \u2248 1/(3.718) \u2248 0.2689.\n\n0: 1/(1+1) = 0.5.\n\n1: exp(-1) is 1/e \u22480.3678. So 1/(1+0.3678) = 1/1.3678 \u22480.7311. Which matches the sample output.\n\nSo the sigmoid seems correct.\n\nPutting it all together, I need to write a function that checks the mode and applies the correct operation.\n\nSo the function will look like this: if mode is 'binary_step', apply the binary step method. Else if 'relu', apply ReLU. Else if 'sigmoid', apply sigmoid.\n\nNow, considering performance. Using numpy's built-in functions is efficient because they're optimized. So the way I have written so far is already optimized for large vectors.\n\nSo now, let me think about the code structure.\n\nThe function is:\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n\nSo, inside the function, the steps are:\n\n- Check the mode.\n\nCase for 'binary_step':\n    return (vector >= 0).astype(int)\n\nCase 'relu':\n    return np.maximum(vector, 0)\n\nCase 'sigmoid':\n    return 1 / (1 + np.exp(-vector))\n\nNow, what about possible edge cases? Like vector containing nothing, but the constraints say it's a 1D array, always. So no need to handle empty vector.\n\nWhat about very large inputs? Well, the exponential function's outputs might be numerically unstable for very large x, but that's inherent to the function, not our code's problem.\n\nWait, for the sigmoid, what if the input is very large and positive? Then exp(-x) becomes zero, so 1/(1+0) = 1. If x is a huge negative, exp(-x) becomes huge, so it tends to zero. So that's correct.\n\nNow, writing the code.\n\nSo putting it all together:\n\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        # According to constraints, mode is always valid, so else might not be needed, but maybe raise error?\n        # But the problem states: mode is always one of the specified, so else: could return as is, but perhaps the constraints mean that we don't need to handle.\n        # But since the options are specified, perhaps it's better to raise an error if it's an unknown mode.\n        # Or per problem statement it's not needed, but in code, it's better to have.\n\n        # So perhaps:\n        raise ValueError(\"Invalid mode.\")\nBut according to the problem statement, the mode will always be one of the specified, so perhaps this is not necessary. But adding it as a safeguard is good practice.\n\nTesting the code.\n\nSample 1: vector = [-1.2, 0, 2, 1.45, -3.7, 0.3], mode 'binary_step'.\nThe code returns [0,1,1,1,0,1], which matches.\n\nSample 2: [-1,0,1,-2,2], 'relu' \u2192 [0,0,1,0,2]. Correct.\n\nSample 3: [-1,0,1], 'sigmoid' \u2192 0.2689,0.5,0.7310... correct.\n\nAnother possible test case: mode 'sigmoid' with a zero vector. For vector[0], it's 0.5, which is correct.\n\nWhat about for more than one dimension? Oh, the function's input is a 1D vector\u2014which is according to the constraints. So code is safe.\n\nAlternative, but wait: How does numpy handle boolean arrays? For example, (vector >= 0) gives a boolean array, which when cast to int becomes 0s and 1s.\n\nTesting that in code, yes.\n\nPotential issues: when vector is not a numpy array. But the function expects vector to be a numpy array, as per the signature.\n\nWait, in the examples, the user is calling the function by passing numpy arrays. So, the code is correct.\n\nBut maybe the function is called with a list? But no, the function's signature says vector is a np.ndarray. So the user is responsible for passing the right type.\n\nSo, all the edge cases are handled.\n\nI think that's correct.\n</think>\n\nTo solve this problem, we need to implement a function that applies different activation functions to a given input vector based on a specified mode. The function should efficiently handle large input vectors using vectorized operations provided by numpy.\n\n### Approach\n1. **Problem Analysis**: The function needs to support three activation functions: Binary Step, ReLU, and Sigmoid. Each activation function is applied element-wise to the input vector.\n2. **Binary Step Function**: This function outputs 1 for non-negative values and 0 otherwise.\n3. **ReLU Function**: This function returns 0 for negative values and the input value for non-negative values.\n4. **Sigmoid Function**: This function maps values to a range between 0 and 1 using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\).\n5. **Optimization**: Using numpy's vectorized operations ensures that the function performs efficiently even for large vectors.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid mode.\")\n```\n\n### Explanation\n- **Binary Step Function**: The function checks each element of the input vector. If the element is non-negative, it is set to 1; otherwise, it is set to 0. This is efficiently computed using boolean indexing and type conversion.\n- **ReLU Function**: This function uses numpy's `maximum` function to set negative values to 0 while leaving non-negative values unchanged.\n- **Sigmoid Function**: This function applies the sigmoid formula element-wise using numpy's exponential function and basic arithmetic operations, ensuring efficient computation.\n\nThis approach ensures that the function is both concise and efficient, leveraging numpy's optimized operations for handling large datasets.",
  "r1_solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid mode.\")",
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm",
    "original_instruction": ""
  },
  "file_source": "KodCode_algorithm_100_1741212598_prepared_results0.jsonl"
}