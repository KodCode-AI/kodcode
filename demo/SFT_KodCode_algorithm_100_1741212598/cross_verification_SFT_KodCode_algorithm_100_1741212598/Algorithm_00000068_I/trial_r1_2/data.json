{
  "style": "instruct",
  "subset": "Algorithm",
  "question_id": "Algorithm_00000068_I",
  "question": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "test_info": [
    {
      "function_name": "activation_function",
      "parameter_list": "(vector, mode)",
      "function_declaration": "def activation_function(vector, mode):",
      "docstring": "Returns the numpy array of the values after applying the specified activation function."
    }
  ],
  "gpt_pass_sequence": [
    0,
    0,
    1
  ],
  "gpt_pass_trial_num": 1,
  "gpt_difficulty": "medium",
  "gpt_pass_percentage": 0.333,
  "r1_response": "<think>\nOkay, I need to solve this problem. The task is to implement a function called activation_function that takes a vector and a mode, and applies the corresponding activation function. So let's plan this step by step.\n\nFirst, I should understand what each activation function does.\n\nThe Binary Step function is straightforward. It outputs 0 for values less than 0 and 1 otherwise. But wait, in one of the examples, the input vector had elements like 0 and it was mapped to 1. So binary step function is 1 for >=0, like that.\n\nThen, ReLU: it's 0 for negative values, and same as input otherwise. So straightforward as well.\n\nSigmoid function: maps each value x to 1/(1 + exp(-x)). So I need to compute this for each element in the vector.\n\nThe function is supposed to work with a numpy array input. So all these operations can be vectorized, which is efficient.\n\nSo the plan is:\n\n1. Check the mode and apply the corresponding activation function.\n\nLet me think about the steps. The function will look something like this:\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.where(vector > 0, vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        # According to constraints, mode is always one of the allowed values, so maybe raise an error else.\n        pass\n\nWait, but for the 'binary_step', the condition is vector >=0, correct. But what about non-integer, non-binary outputs. For example, the vector elements could be floating points, but after binary_step, all elements are 0 or 1.\n\nBut looking at the sample input:\n\nFor example, in binary_step example, vector is [-1.2, 0, 2, 1.45, -3.7, 0.3]. The output is [0,1,1,1,0,1]. So 0 is mapped to 1, which matches vector >=0.\n\nYes, so for binary_step, condition is vector >=0.\n\nWait, for the value exactly 0, it's included in >=0, so correct.\n\nNow, in the ReLU case, any positive or zero? Wait, the ReLU function typically sets negative values (x<0), so x<0 becomes 0. So in code, it's where the vector is less than zero, replace with 0, else keep as is.\n\nWait, what about zero in ReLU? ReLU is max(0, x), so when x is zero or positive, the value remains. So in code: where vector is less than zero, set to 0; else keep.\n\nSo in code: np.where(vector < 0, 0, vector).\n\nWait, in the sample given, for ReLU, the input vector is [-1, 0,1,-2,2]. The output is [0,0,1,0,2].\n\nWait, in the sample, when the input is 0, output is 0. Which matches with the ReLU definition. So yes.\n\nSo in code:\n\nFor 'relu':\napply an element-wise condition where vector <0, set to 0, else vector.\n\nWait, but looking back: When I initalu thought it's where vector >0, now I'm thinking it's vector <0. So which is correct.\n\nWait, let's see.\n\nThe ReLU function is max(0, x). So for x negative, it's 0. For x positive, same as x. So in code:\n\nFor each element x in vector:\n\nif x < 0 , output 0 else output x.\n\nSo yes, that's correct.\n\nThus, for code, np.where(vector <0, 0, vector).\n\nBut wait, looking at that.\n\nLet me test with the sample:\n\nvector = np.array([-1,0,1,-2,2]), when mode is 'relu'.\n\nAfter applying vector <0, the elements would be True, False, False, True, False.\n\nThus, np.where applies 0 where True, else the value. So for the first element, it's set to 0, for the second element, since vector=0, vector<0 is False, so output is 0? Wait no.\n\nWait, vector <0 for 0 is False, so vector <0, 0 is applied only to where True. In else, apply the value.\n\nWait, no. For ReLU, 0 is mapped to 0.\n\nWait, yes. Because 0 is not less than 0. So for the vector element 0, ReLU is max(0,0) which is 0. So the sample has 0 in the input and 0 in the output. So the code I have now, which uses vector <0, will lead 0 to be kept in output.\n\nWait, but in code, if I have np.where(vector <0, 0, vector), then vector=0 is False, so it's replaced with the original vector value, which is 0. So yes, that's correct.\n\nAlternatively, another approach is to compute np.maximum(vector, 0), which automatically sets all elements <=0 to 0 and positive to their value. So that's a more efficient way perhaps in numpy.\n\nSimilarly, for binary_step: np.where(vector >=0, 1, 0).\n\nSo in the code:\n\n- For 'binary_step', as mentioned.\n\n- For 'relu', use np.maximum(vector, 0) or the where approach.\n\nBut for performance, vectorized operations like np.maximum are optimal.\n\nWhich would be better for performance? Using np.where or the built-in functions.\n\nHmm. For maximum, np.maximum is a built-in ufunc, and is very efficient. So perhaps for 'relu', using np.maximum(vector, 0) is better.\n\nSimilarly for 'binary_step', perhaps np.where is a good approach.\n\nBut for 'sigmoid', I have to compute 1/(1 + np.exp(-x)). Since np.exp is a ufunc in numpy, applying it on the vector will be efficient.\n\nNow, implementing each case:\n\nCase 1: mode = 'binary_step'\noperation: vector >=0 ? 1 : 0\nso return np.where(vector >= 0, 1, 0)\n\nCase 2: mode = 'relu'\noperation: max(0, vector)\nso return np.maximum(vector, 0)\n\nCase 3: mode = 'sigmoid'\noperation: 1/(1 + exp(-x))\nso 1 / (1 + np.exp(-vector))\n\nSo putting this together.\n\nLooking at the sample inputs:\n\nSample 1:\n\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nmode = 'binary_step'\n\noutput:\n\nnp.where(vector >=0, 1, 0) would create [0, 1,1,1,0,1]. Which matches.\n\nSample 2:\n\nvector = np.array([-1, 0, 1, -2, 2])\nmode = 'relu'\n\nUsing np.maximum: [-1 would be 0, 0 becomes 0, 1 remains 1, -2 \u21920, 2\u21922. So matches.\n\nSample3:\n\nvector = [-1,0,1], mode='sigmoid'.\n\nCompute 1/(1+exp(1)), exp(-1)=0.36788 so 1/(1+... )\n\nSo for x=-1: exp(1) = e^1\u22482.718, 1+e= 3.718, 1/(3.718)\u22480.2689.\n\nFor x=0: exp(0)=1, 1+1=2, 1/2=0.5.\n\nFor x=1: exp(-1)\u22480.3679, 1/(1+0.3679)\u22480.73106.\n\nWhich is correct.\n\nSo all looks well.\n\nNow, putting it all into code.\n\nImplement the function:\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        # Wait according to constraints, mode is one of the strings, so this else may not be necessary, but perhaps it's better to raise a ValueError instead.\n        raise ValueError(\"Invalid mode.\")\n\nWait, for the else case, since the problem says mode is one of the three, perhaps we can assume no invalid modes are passed, but perhaps the user would pass a typo, like 'relU' instead of 'relu'. But according to the constraints, the mode is always one of the specified strings, so that's probably not an issue here.\n\nBut anyway, it's safer to include it.\n\nSo the code is as written.\n\nLet me check the examples.\n\nSample 1: Should output [0,1,1,1,0,1]. Correct.\n\nSample 2: Output is [0,0,1,0,2] --> yes, since ReLU returns max(0,x) which sets negative x to 0, leaves others as is.\n\nSample3: Correct.\n\nSo the code should handle the cases correctly.\n\nIn terms of performance, the function uses numpy's optimized operations which are written in C, so it's efficient even for large vectors.\n\nSo that should do it.\n</think>\n\nTo solve this problem, we need to implement a function that applies different activation functions to a given input vector based on a specified mode. The function should handle three types of activation functions: Binary Step, ReLU, and Sigmoid. Each activation function is applied element-wise to the input vector.\n\n### Approach\n1. **Binary Step Function**: This function outputs 1 for all non-negative values and 0 for negative values. This is implemented using numpy's `where` function which allows vectorized conditional operations.\n2. **ReLU Function**: This function returns 0 for all negative values and the input value for non-negative values. This can be efficiently implemented using numpy's `maximum` function, which computes the element-wise maximum of the input vector and 0.\n3. **Sigmoid Function**: This function maps each value to a range between 0 and 1 using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This is implemented using numpy's `exp` function for exponentiation and is applied element-wise.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid mode.\")\n```\n\n### Explanation\nThe function `activation_function` takes two parameters: a numpy array `vector` and a string `mode`. It applies the activation function specified by `mode` to each element of `vector`.\n\n1. **Binary Step Mode**: For each element in the input vector, if the value is greater than or equal to 0, it is set to 1; otherwise, it is set to 0.\n2. **ReLU Mode**: For each element in the input vector, if the value is negative, it is set to 0; otherwise, it remains unchanged.\n3. **Sigmoid Mode**: Each element in the input vector is transformed using the sigmoid function to a value between 0 and 1.\n\nThe function efficiently handles large input vectors using numpy's optimized operations, ensuring performance is maximized.",
  "r1_solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid mode.\")",
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm",
    "original_instruction": ""
  },
  "file_source": "KodCode_algorithm_100_1741212598_prepared_results2.jsonl"
}