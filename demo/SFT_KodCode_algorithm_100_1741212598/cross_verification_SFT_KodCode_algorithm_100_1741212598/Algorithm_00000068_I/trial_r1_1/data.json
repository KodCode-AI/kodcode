{
  "style": "instruct",
  "subset": "Algorithm",
  "question_id": "Algorithm_00000068_I",
  "question": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "test_info": [
    {
      "function_name": "activation_function",
      "parameter_list": "(vector, mode)",
      "function_declaration": "def activation_function(vector, mode):",
      "docstring": "Returns the numpy array of the values after applying the specified activation function."
    }
  ],
  "gpt_pass_sequence": [
    0,
    0,
    1
  ],
  "gpt_pass_trial_num": 1,
  "gpt_difficulty": "medium",
  "gpt_pass_percentage": 0.333,
  "r1_response": "<think>\nOkay, I need to implement this activation_function in Python. So, the function takes a vector and a mode, and applies the corresponding activation function. Let's think through each part.\n\nFirst, let's understand each activation function.\n\nThe binary step function returns 1 if the input is >=0, else 0. So for each element in the vector, if it's positive or zero, set to 1; else 0.\n\nReLU is straightforward: returns 0 for negative values, else the input. So elements <0 become 0, others stay the same.\n\nSigmoid is a bit more complex. It's 1/(1 + e^(-x)). So for each element, compute that function.\n\nBut how to do this efficiently with numpy for large vectors? Oh right, numpy allows vectorized operations.\n\nSo, the plan is:\n\n1. Check the mode parameter.\n2. Depending on mode, apply the corresponding function across the vector, element-wise.\n3. Return the transformed vector.\n\nLet's think about each mode.\n\nFor 'binary_step': Use a threshold. We can create a boolean array where elements >=0 are True, else False. Then, cast True to 1 and False to 0.\n\nWait, but in numpy, you can do (vector >= 0).astype(int) for binary step. Or maybe the other way, where vector > 0 is 1, else 0. But according to the example, if input is 0, it's 1. So >=0 is correct.\n\nFor 'relu': Use np.maximum(vector, 0). Or create a mask where vector is negative, set those to 0.\n\nFor 'sigmoid': Compute using the function. Numpy can handle exponentials, so we can compute 1/(1 + np.exp(-vector)).\n\nSo the steps are:\n\nIf mode is 'binary_step', compute (vector >= 0).astype(int)\nIf 'relu', compute np.maximum(vector, 0)\nIf 'sigmoid', compute 1/(1 + np.exp(-vector))\n\nWait, but is there any issue with data types? Like for very large numbers, but numpy handles that.\n\nSo in code:\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        # return 1 where vector >=0, else 0\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        # According to the problem, mode is always one of the given, so this might not be needed.\n        raise ValueError(\"Invalid mode\")\n\nWait, but for better performance with large vectors, are there any optimizations? I think using numpy's vectorized operations is already efficient, so this should be fine.\n\nTesting examples:\n\nFirst example: vector is [-1.2, 0, 2, 1.45, -3.7, 0.3]\nbinary step: all negatives become 0, zeros and positives 1. So [0,1,1,1,0,1] as output. Which matches the example.\n\nSecond example: vector [-1, 0,1,-2,2], mode 'relu' gives [0,0,1,0,2]. Which is correct.\n\nThird example: vector [-1, 0,1], 'sigmoid':\n\nCalculate for each x:\nx = -1: 1/(1 + e^(1)) = 1/(1 + 2.718..) \u2248 0.2689\nx=0: 1/(1+1) = 0.5\nx=1: 1/(1+ e^-1) \u2248 1/(1 + 0.3678) \u2248 0.73105\n\nYes, that's correct.\n\nSo, what about vector types? Numpy arrays are the inputs, and the function returns a modified numpy array.\n\nSo the code should handle all these correctly.\n\nI think this should work.\n</think>\n\nTo solve this problem, we need to implement a generalized activation function for a neural network layer. The function should be able to switch between different activation functions based on a specified mode. The activation functions we need to support are the Binary Step function, ReLU, and Sigmoid.\n\n### Approach\nThe task is to create a function `activation_function` that takes a numpy array (vector) and a mode (string) as inputs and applies the corresponding activation function to the vector. Here's the step-by-step approach:\n\n1. **Binary Step Function**: This function returns 1 for all non-negative values and 0 for negative values. Using numpy's boolean operations, we can efficiently compute this as `(vector >= 0).astype(int)`, which converts the boolean array to integers.\n\n2. **ReLU Function**: The Rectified Linear Unit (ReLU) function returns 0 for negative values and the input value for non-negative values. This can be efficiently computed using numpy's `maximum` function: `np.maximum(vector, 0)`.\n\n3. **Sigmoid Function**: The sigmoid function maps the input to a range between 0 and 1 using the formula \\( \\frac{1}{1 + e^{-x}} \\). Numpy's vectorized operations allow us to compute this efficiently for the entire array at once.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")\n```\n\n### Explanation\n- **Binary Step Function**: This function is implemented by checking each element of the vector to see if it is non-negative, converting the result to an integer array where non-negative values are 1 and negative values are 0.\n\n- **ReLU Function**: Using numpy's `np.maximum` function, we ensure that all negative values in the vector are set to 0 while non-negative values remain unchanged.\n\n- **Sigmoid Function**: This function is computed using numpy's exponential function `np.exp` to apply the transformation \\( \\frac{1}{1 + e^{-x}} \\) across the entire vector efficiently.\n\nThe function is designed to handle large input vectors efficiently by leveraging numpy's optimized, vectorized operations, ensuring good performance even for large datasets. Each activation function is applied in a single, vectorized operation to maintain efficiency, making the solution both concise and high-performing.",
  "r1_solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")",
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm",
    "original_instruction": ""
  },
  "file_source": "KodCode_algorithm_100_1741212598_prepared_results1.jsonl"
}