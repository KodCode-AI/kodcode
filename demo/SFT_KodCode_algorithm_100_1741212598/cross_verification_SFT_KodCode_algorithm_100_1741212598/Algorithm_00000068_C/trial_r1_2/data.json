{
  "style": "instruct",
  "subset": "Algorithm",
  "question_id": "Algorithm_00000068_C",
  "question": "def activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    pass\n\ndef test_activation_function():\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\nif __name__ == \"__main__\":\n    test_activation_function()",
  "solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "test_info": [
    {
      "function_name": "activation_function",
      "parameter_list": "(vector, mode)",
      "function_declaration": "def activation_function(vector, mode):",
      "docstring": "Returns the numpy array of the values after applying the specified activation function."
    }
  ],
  "gpt_pass_sequence": [
    0,
    0,
    1
  ],
  "gpt_pass_trial_num": 1,
  "gpt_difficulty": "medium",
  "gpt_pass_percentage": 0.333,
  "r1_response": "<think>\nOkay, I need to implement the activation_function as per the given code. The function takes a numpy array and a mode string, and returns the array after applying the specified activation function.\n\nLooking at the test cases, there are different modes: 'binary_step', 'relu', 'sigmoid', and an 'unknown' case which should raise an error.\n\nFirst, I should think about each activation function.\n\nBinary Step: This function outputs 0 for values less than 0, and 1 otherwise. So for each element in the vector, if it's >=0, set to 1, else 0.\n\nReLU: This stands for Rectified Linear Unit. It returns 0 for values less than 0, and the same value otherwise. So for each element, if it's <0, output 0, else output the element.\n\nSigmoid: This function uses the sigmoid formula, which is 1/(1 + e^(-x)). So I need to compute this for each element.\n\nNow, I need to handle the mode parameter.\n\nThe function should apply the corresponding function based on the mode. So I'll have a conditional check: if mode is 'binary_step', apply binary step. Else if mode is 'relu', apply ReLU. Else if mode is 'sigmoid', apply sigmoid. Else, raise a ValueError.\n\nLet me outline the code structure.\n\nWait, but how do I compute these efficiently, given that the input is a numpy array?\n\nFor binary step, I can use np.where(vector >=0, 1, 0). But wait, sometimes the binary step is defined as 1 when x>0, and 0 otherwise, including at x=0. Or sometimes it's 1 when x>=0, including zero. Looking at the test case:\n\nIn the first test case, the input vector has 0, and the expected output is 1 in that position. So yes, in 'binary_step' mode, 0 is mapped to 1. So binary_step is 1 for x >=0, 0 otherwise.\n\nSo, for binary_step: result = np.where(vector >= 0, 1, 0). But wait, another test case: vector -1.2 becomes 0, 0 becomes 1, that matches.\n\nFor ReLU, same approach: result is vector where elements <0 are 0, else same as input.\n\nNow, for sigmoid: How to compute this in numpy? I can compute it as 1/(1 + np.exp(-vector)). I need to import numpy, but since the function uses np.ndarray as input, I assume numpy is already imported.\n\nWait, but the code provided doesn't import numpy. Hmm, but the function is given a vector as np.ndarray, so I think in the actual code, I should import numpy.\n\nWait, the code outline given already includes the numpy import, right? Because the function is written as vector: np.ndarray, which implies numpy is imported as np.\n\nSo I can proceed assuming that.\n\nSo, plan:\n\n_activation_function function:\n\n- Check the mode.\n- For each mode, compute the activation.\n\nEdge cases: what if mode is something else, like 'tanh' or another function? The test expects that raising a ValueError is appropriate.\n\nSo, first handle the mode, and in else case, raise ValueError.\n\nLet me write it in code.\n\nWait, but let me look at the test cases.\n\nThe first test: vector is [-1.2, 0, 2, 1.45, -3.7, 0.3]. For 'binary_step', output is [0,1,1,1,0,1].\n\nSo that's correct as per binary_step applied.\n\nSecond test: vector [-1,0,1,-2,2], mode 'relu': expected [0,0,1,0,2]. So yes, ReLU applied.\n\nThird test: vector [-1,0,1], mode 'sigmoid', and the expected is expit, which is from scipy. It's the same as 1/(1+e^(-x)).\n\nSo the code for the function would look like:\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.where(vector < 0, 0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")\n\nWait, let me test this code against the given tests.\n\nFirst test:\n\nvector = [-1.2, 0, 2, 1.45, -3.7, 0.3]\nmode binary_step, expected [0,1,1,1,0,1].\n\nIn binary_step, for each element:\n\n-1.2: 0, 0:1, 2:1, 1.45:1, -3.7:0, 0.3:1. So yes.\n\nSecond test: vector [-1,0,1,-2,2], mode 'relu' expected [0,0,1,0,2].\n\nYes:\n\n-1 => 0, 0=>0, 1=>1, -2 =>0, 2=>2.\n\nThird test: vector [-1,0,1], mode 'sigmoid'.\n\nThe code computes 1/(1 + exp(-x)), which is the correct formula. For x=-1: 1/(1+exp(1)) \u2248 0.2689, etc. Comparing to expit which does the same.\n\nThe function should return these correctly.\n\nNow, what about if the mode is 'unknown'? The function raises a ValueError, which is caught in the test.\n\nI think that's correct.\n\nWait, but wait for the 'relu' function: in the code I have, np.where(vector<0, 0, vector). So for elements less than zero, set to 0, else keep as is. Yes, that's correct.\n\nBut wait, in numpy's where function, the syntax is np.where(condition, x, y). So condition is vector >=0 for binary_step: if yes, x=1, else y=0. Correct.\n\nAnother point: Are there any modes that need to be handled differently, like 'tanh'? Not per the tests, because any mode other than the three is raising an error, and the fourth test checks that.\n\nSo the code seems correct.\n\nWait, but what about if the vector is a 2D array or higher dimensions? The function should still work as the activation is applied element-wise, which numpy handles automatically.\n\nBut since the test cases pass vectors (1D arrays), code should be fine.\n\nAlso, for 'sigmoid', the code uses 1/(1 + np.exp(-vector)), which is the correct implementation.\n\nSo I think the code is solid.\n</think>\n\nTo solve this problem, we need to implement a function that applies a specified activation function to each element of a given numpy array. Activation functions are crucial in neural networks as they introduce non-linearity into the model, enabling it to learn more complex patterns.\n\n### Approach\n\nThe approach involves creating a function `activation_function` that takes two parameters: a numpy array (`vector`) and a string (`mode`). The function applies the activation function specified by `mode` to each element of the input array and returns the resulting numpy array.\n\n1. **Binary Step Activation**: This function outputs 1 for non-negative values and 0 for negative values. It can be implemented using numpy's `where` function to check if each element is greater than or equal to 0 and set it to 1 or 0 accordingly.\n\n2. **ReLU (Rectified Linear Unit) Activation**: This function outputs 0 for negative values and the value itself for non-negative values. Again, using numpy's `where` function, we can check if each element is less than 0 and set it to 0, otherwise, retain the value.\n\n3. **Sigmoid Activation**: This function maps any real-valued number to a value between 0 and 1 using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). We can compute this using numpy's exponential function.\n\n4. **Error Handling**: If an unknown activation mode is provided, the function raises a ValueError to indicate an invalid mode.\n\n### Solution Code\n\n```python\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of after applying the specified activation function.\n    \n    Parameters:\n    vector (np.ndarray): Input array to apply the activation function.\n    mode (str): The activation function to apply. Can be 'binary_step', 'relu', or 'sigmoid'.\n    \n    Returns:\n    np.ndarray: Array after applying the activation function.\n    \n    Raises:\n    ValueError: If the mode is not one of the specified activation functions.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.where(vector < 0, 0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(f\"Invalid activation mode: {mode}.\")\n```\n\n### Explanation\n\n- **Binary Step Activation**: The function checks each element of the input array. If the element is greater than or equal to 0, it sets the output to 1; otherwise, it sets the output to 0.\n\n- **ReLU Activation**: This function checks each element of the input array. If the element is less than 0, it sets the output to 0; otherwise, it retains the element's value.\n\n- **Sigmoid Activation**: This function computes the sigmoid of each element using the formula \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\). This is implemented efficiently using numpy operations.\n\n- **Error Handling**: If the provided mode is not one of the supported activation functions, the function raises a ValueError with an appropriate message.\n\nThis approach ensures that each element of the input array is processed efficiently using vectorized operations, making the function both concise and performant.",
  "r1_solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of after applying the specified activation function.\n    \n    Parameters:\n    vector (np.ndarray): Input array to apply the activation function.\n    mode (str): The activation function to apply. Can be 'binary_step', 'relu', or 'sigmoid'.\n    \n    Returns:\n    np.ndarray: Array after applying the activation function.\n    \n    Raises:\n    ValueError: If the mode is not one of the specified activation functions.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.where(vector < 0, 0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(f\"Invalid activation mode: {mode}.\")",
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm",
    "original_instruction": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```"
  },
  "file_source": "KodCode_algorithm_100_1741212598_prepared_results2.jsonl"
}