{
  "style": "instruct",
  "subset": "Algorithm",
  "question_id": "Algorithm_00000068_C",
  "question": "def activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    pass\n\ndef test_activation_function():\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\nif __name__ == \"__main__\":\n    test_activation_function()",
  "solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "test_info": [
    {
      "function_name": "activation_function",
      "parameter_list": "(vector, mode)",
      "function_declaration": "def activation_function(vector, mode):",
      "docstring": "Returns the numpy array of the values after applying the specified activation function."
    }
  ],
  "gpt_pass_sequence": [
    0,
    0,
    1
  ],
  "gpt_pass_trial_num": 1,
  "gpt_difficulty": "medium",
  "gpt_pass_percentage": 0.333,
  "r1_response": "<think>\nAlright, I need to solve this problem where I have to implement the activation_function for a neural network. The function takes a vector and a mode, which specifies the activation function to apply. The test cases are already provided, so I should make sure my code passes them.\n\nAlright, let's start by understanding the function. The activation_function is supposed to apply different activation functions based on the 'mode' parameter. The possible modes, as I can see from the test cases, are 'binary_step', 'relu', 'sigmoid', and if it's something else, it raises a ValueError.\n\nWait, looking at the test cases: the first test uses 'binary_step', the second 'relu', the third 'sigmoid', and the last one tries 'unknown', expecting a ValueError. So, I need to handle these cases.\n\nSo, I think I need to write a function that, for each element in the input vector, applies the activation function based on the mode.\n\nFirst, the function is defined as taking a numpy array vector and a string mode. And it should return a numpy array.\n\nHmm. For each mode, I'll have to process each element.\n\nLet's think about each mode:\n\n1. 'binary_step': I remember that the binary step function, also known as the Heaviside step function, returns 0 if the input is less than 0, and 1 otherwise. So for each element in the vector, if it's negative, output 0, else 1. But wait, sometimes the step function returns 0 for negative and 1 for zero and positive. So I think that's the case here. But let me check the test case.\n\nLooking at the first test case:\n\nvector = [-1.2, 0, 2, 1.45, -3.7, 0.3]\nexpected = [0, 1, 1, 1, 0, 1]\n\nSo yes, any value >=0 returns 1, otherwise 0. So binary_step is the same as the Heaviside step.\n\n2. 'relu': The rectified linear unit. It returns 0 for negative numbers and the same number for positives. So, any x < 0 becomes 0, else x. Wait, let me look at the test case. vector is [-1, 0, 1, -2, 2], expected is [0, 0, 1, 0, 2]. Yep, correct.\n\n3. 'sigmoid': The sigmoid function. The formula is 1/(1 + e^(-x)), which outputs a value between 0 and 1. The third test case uses [ -1, 0, 1 ] and compares with scipy's expit function, which is the same as sigmoid. So, I need to compute this correctly.\n\nSo, the function for the sigmoid should compute 1/(1 + np.exp(-x)).\n\nNow, how to structure this.\n\nI think the activation_function will check the mode, then apply the corresponding function element-wise.\n\nI can approach this step by step.\n\nFirst, the function returns a numpy array. So, for each mode, compute a transformed version of the vector.\n\nLet me outline steps:\n\n1. Handle each mode in separate cases.\n\n2. For each element in vector, compute based on the mode.\n\nWait, but since numpy is vectorized, perhaps I can avoid loops and compute it more efficiently using vector operations.\n\nOkay, for the 'binary_step' mode. I can create a boolean array where (vector >= 0) is True, then cast it to int, which gives 1s and 0s. So:\n\nbinary_step = (vector >= 0).astype(int)\n\nWait, but let me test if in numpy, 0 is considered as True. Wait, in Python, 0 is considered False in a boolean context. So for the vector value 0, the comparison vector >=0 will give True, so the result is 1. Which is correct because the test case expects 1 for 0.\n\nWait in the first test case, the vector has 0 and according to the expected output, it's 1. So binary_step correctly includes 0 as 1.\n\nSo, that's correct.\n\nFor 'relu', it's simply np.maximum(vector, 0). Because for each x, relu(x) is max(0, x). So in vector form, we can compute it as vector * (vector > 0) or use np.maximum.\n\nYes, np.maximum(vector, 0) is efficient and correct.\n\nFor 'sigmoid', as I said, compute 1/(1 + np.exp(-x)). So, I can compute it using vector operations:\n\nsigmoid = 1 / (1 + np.exp(-vector))\n\nBut numpy handles element-wise exponentials, so that's okay.\n\nAs for the 'unknown' mode, the function should raise a ValueError. So, the function should first check if the mode is supported. Maybe we have a list of allowed modes and if the given mode is not in that list, raise an error.\n\nSo, putting it all together.\n\nThe function could look like this:\n\nCheck the mode first. If mode is 'binary_step', apply the step function as above.\n\nIf mode is 'relu', apply np.maximum(vector, 0).\n\nIf mode is 'sigmoid', compute the sigmoid function.\n\nElse, raise a ValueError.\n\nWait, but how about the efficiency? Since each operation is vectorized, it's efficient.\n\nSo now, coding this.\n\nWait, but let's test it with the provided test cases.\n\nTest case 1: 'binary_step'\n\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nexpected = np.array([0, 1, 1, 1, 0, 1])\n\nCompute (vector >=0).astype(int) gives:\n\nvector >=0 is [False, True, True, True, False, True]\n\nso [0,1,1,1,0,1]. So correct.\n\nTest case 2: 'relu'\n\nvector = np.array([-1, 0, 1, -2, 2])\n\nnp.maximum(vector, 0) is [0,0,1,0,2]. Which matches the expected [0, 0, 1, 0, 2]. Correct.\n\nTest case 3: 'sigmoid'\n\nvector = np.array([-1, 0, 1])\n\nsigmoid function is 1/(1 + e^(-x)).\n\nAt x=-1, it's 1/(1 + e) = 1/(1+2.71828...) \u2248 0.2689.\n\nexpit is the same as scipy.special.expit, which is the exact implementation.\n\nSo the code correctly computes this, and the test case checks for np.allclose, which is correct.\n\nNow, implementing all these.\n\nSo the code will be:\n\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")\n\nWait, but what about the cases where mode is not any of these? The last test case expects a ValueError when 'unknown' is passed.\n\nYes, so the else clause will raise the ValueError.\n\nWait, but the first line in the function should be:\n\nFirst, check the mode.\n\nYes, the code seems to handle it.\n\nBut wait, in the third test case, they compute expit(vector) where expit is from scipy. So the function should return the same as 1/(1+exp(-x)) for each x in the vector. Which is exactly what our code does.\n\nSo the code should pass the test.\n\nI think that's all. So the function is implemented as above.\n</think>\n\nTo solve this problem, we need to implement an activation function for a neural network that can apply different types of activation based on the specified mode. The function takes a vector and a mode as inputs and returns the transformed vector after applying the specified activation function.\n\n### Approach\nThe approach involves handling different activation functions in separate cases. The function checks the mode and applies the corresponding activation function to each element of the input vector. The supported activation functions are:\n\n1. **Binary Step**: This function returns 0 for negative values and 1 for non-negative values.\n2. **ReLU (Rectified Linear Unit)**: This function returns 0 for negative values and the value itself for non-negative values.\n3. **Sigmoid**: This function maps the input to a value between 0 and 1 using the formula \\( \\frac{1}{1 + e^{-x}} \\).\n\nIf an unsupported mode is provided, the function raises a ValueError.\n\n### Solution Code\n```python\nimport numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")\n```\n\n### Explanation\n- **Binary Step**: The binary step function is implemented by checking if each element is non-negative, returning 1 if true and 0 otherwise. This is efficiently done using numpy's boolean operations and casting the result to integers.\n- **ReLU**: The ReLU function is implemented using numpy's `maximum` function, which returns 0 for negative values and the value itself for non-negative values.\n- **Sigmoid**: The sigmoid function is computed using the formula \\( \\frac{1}{1 + e^{-x}} \\), leveraging numpy's vectorized operations for efficiency.\n\nThis approach ensures that each activation function is applied efficiently and correctly, handling each mode separately and raising an error for unsupported modes.",
  "r1_solution": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return (vector >= 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(vector, 0)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Invalid activation mode.\")",
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm",
    "original_instruction": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```"
  },
  "file_source": "KodCode_algorithm_100_1741212598_prepared_results0.jsonl"
}