{
  "metadata": {
    "prompt_id": "00000028",
    "row_id": 28,
    "seed_ids": [
      112
    ],
    "mode": "algorithm"
  },
  "instruction": "### Question\n\nYou are working on a project to predict the `Rating` of players based on their `ADR` (Average Damage Per Round) in a multiplayer game. You have collected the data and want to use linear regression to model the relationship between `ADR` and `Rating`.\n\n**Task**:\nImplement the `run_linear_regression` function to perform linear regression on the provided dataset. Additionally, enhance the model by implementing `L2 regularization` to prevent overfitting.\n\n**Input**:\n- `data_x`: A matrix of shape `(n_samples, n_features + 1)` where each row represents a sample and the first column is a bias term.\n- `data_y`: A vector of shape `(n_samples,)` containing the corresponding target values.\n\n**Output**:\n- `theta`: A vector of shape `(1, n_features + 1)` representing the feature vector of the line of best fit.\n\n**Constraints**:\n- The learning rate `alpha` should be `0.0001550`.\n- The number of iterations should be `100000`.\n- Regularization parameter `lambda` should be `0.01`.\n\n**Performance Requirement**:\n- The implementation should be efficient and avoid overfitting.\n\n**Helper Functions**:\n- `run_steep_gradient_descent`: Performs a single step of gradient descent.\n- `sum_of_square_error`: Computes the sum of square error.\n- `mean_absolute_error`: Computes the mean absolute error.\n\n**Note**:\n- Ensure that your implementation is self-contained and does not reference the provided code snippet directly.\n- Test your implementation using the provided dataset.\n\n### Example\n\n```python\ndata_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\ndata_y = np.array([4, 5, 6, 7])\ntheta = run_linear_regression(data_x, data_y)\nprint(theta)  # Should print approximately [4.0, 0.01]\n```\n\n### Scoring\n\n- **Correct Implementation**: 70 points.\n- **Efficient Implementation (Regularization)**: 30 points.",
  "solution_code": "import numpy as np\n\ndef run_linear_regression(data_x, data_y):\n    \"\"\"\n    Perform linear regression with L2 regularization.\n    \n    Parameters:\n    - data_x: A matrix of shape (n_samples, n_features + 1)\n    - data_y: A vector of shape (n_samples,)\n    \n    Returns:\n    - theta: A vector of shape (1, n_features + 1)\n    \"\"\"\n    alpha = 0.0001550\n    num_iterations = 100000\n    lambda_ = 0.01\n    n_samples, n_features = data_x.shape\n    theta = np.zeros((1, n_features))\n    \n    for _ in range(num_iterations):\n        predictions = np.dot(data_x, theta.T)\n        error = predictions - data_y\n        gradient = np.dot(data_x.T, error) / n_samples + lambda_ * theta\n        theta -= alpha * gradient\n    \n    return theta\n\ndef run_steep_gradient_descent(data_x, data_y, theta, alpha, num_iterations):\n    \"\"\"\n    Perform gradient descent for linear regression.\n    \"\"\"\n    for _ in range(num_iterations):\n        predictions = np.dot(data_x, theta.T)\n        error = predictions - data_y\n        gradient = np.dot(data_x.T, error) / len(data_y)\n        theta -= alpha * gradient\n    return theta\n\ndef sum_of_square_error(data_x, data_y, theta):\n    \"\"\"\n    Compute the sum of square error.\n    \"\"\"\n    predictions = np.dot(data_x, theta.T)\n    return np.sum((predictions - data_y) ** 2)\n\ndef mean_absolute_error(data_x, data_y, theta):\n    \"\"\"\n    Compute the mean absolute error.\n    \"\"\"\n    predictions = np.dot(data_x, theta.T)\n    return np.mean(np.abs(predictions - data_y))\n\n# Initialize data\ndata_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\ndata_y = np.array([4, 5, 6, 7])\n\n# Run the linear regression model\ntheta = run_linear_regression(data_x, data_y)\nprint(theta)",
  "test_code": "import numpy as np\nfrom solution import run_linear_regression, run_steep_gradient_descent, sum_of_square_error, mean_absolute_error\n\n# Test cases\ndef test_run_linear_regression():\n    # Test with a simple dataset\n    data_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\n    data_y = np.array([4, 5, 6, 7])\n    theta = run_linear_regression(data_x, data_y)\n    # Expected theta values can be based on the model fitting and validation\n    assert np.isclose(theta[0, 0], 4.0, atol=0.01)\n    assert np.isclose(theta[0, 1], 0.01, atol=0.001)\n\ndef test_sum_of_square_error():\n    # Example data\n    data_x = np.array([[1, 2], [1, 3], [1, 4]])\n    data_y = np.array([2, 3, 4])\n    theta = np.array([[1, 1]])\n    assert np.isclose(sum_of_square_error(data_x, data_y, theta), 0.0)\n\ndef test_mean_absolute_error():\n    # Example data\n    data_x = np.array([[1, 2], [1, 3], [1, 4]])\n    data_y = np.array([2, 3, 4])\n    theta = np.array([[1, 1]])\n    assert np.isclose(mean_absolute_error(data_x, data_y, theta), 0.0)\n\n# Run the test functions\nif __name__ == \"__main__\":\n    test_run_linear_regression()\n    test_sum_of_square_error()\n    test_mean_absolute_error()",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results2.jsonl"
}