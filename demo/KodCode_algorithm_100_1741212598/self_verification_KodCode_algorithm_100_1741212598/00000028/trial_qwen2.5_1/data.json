{
  "metadata": {
    "prompt_id": "00000028",
    "row_id": 28,
    "seed_ids": [
      112
    ],
    "mode": "algorithm"
  },
  "instruction": "### Question\n\nYou are working on a project to predict the `Rating` of players based on their `ADR` (Average Damage Per Round) in a multiplayer game. You have collected the data and want to use linear regression to model the relationship between `ADR` and `Rating`.\n\n**Task**:\nImplement the `run_linear_regression` function to perform linear regression on the provided dataset. Additionally, enhance the model by implementing `L2 regularization` to prevent overfitting.\n\n**Input**:\n- `data_x`: A matrix of shape `(n_samples, n_features + 1)` where each row represents a sample and the first column is a bias term.\n- `data_y`: A vector of shape `(n_samples,)` containing the corresponding target values.\n\n**Output**:\n- `theta`: A vector of shape `(1, n_features + 1)` representing the feature vector of the line of best fit.\n\n**Constraints**:\n- The learning rate `alpha` should be `0.0001550`.\n- The number of iterations should be `100000`.\n- Regularization parameter `lambda` should be `0.01`.\n\n**Performance Requirement**:\n- The implementation should be efficient and avoid overfitting.\n\n**Helper Functions**:\n- `run_steep_gradient_descent`: Performs a single step of gradient descent.\n- `sum_of_square_error`: Computes the sum of square error.\n- `mean_absolute_error`: Computes the mean absolute error.\n\n**Note**:\n- Ensure that your implementation is self-contained and does not reference the provided code snippet directly.\n- Test your implementation using the provided dataset.\n\n### Example\n\n```python\ndata_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\ndata_y = np.array([4, 5, 6, 7])\ntheta = run_linear_regression(data_x, data_y)\nprint(theta)  # Should print approximately [4.0, 0.01]\n```\n\n### Scoring\n\n- **Correct Implementation**: 70 points.\n- **Efficient Implementation (Regularization)**: 30 points.",
  "solution_code": "import numpy as np\n\ndef run_linear_regression(data_x, data_y):\n    \"\"\"\n    Performs linear regression with L2 regularization on the provided data.\n    \"\"\"\n    alpha = 0.0001550\n    lambda_ = 0.01\n    num_iterations = 100000\n    n_samples, n_features = data_x.shape\n    \n    # Initialize theta\n    theta = np.zeros((1, n_features + 1))\n    \n    for _ in range(num_iterations):\n        gradient = np.dot(data_x.T, (np.dot(data_x, theta) - data_y)) / n_samples + \\\n                   (2 * lambda_ / n_samples) * theta\n        theta -= alpha * gradient\n    \n    return theta\n\n# Helper functions for testing\ndef sum_of_square_error(y_true, y_pred):\n    return np.sum((y_true - y_pred) ** 2)\n\ndef mean_absolute_error(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\ndef run_steep_gradient_descent(data_x, data_y, theta, alpha, num_iterations):\n    n_samples, n_features = data_x.shape\n    for _ in range(num_iterations):\n        gradient = np.dot(data_x.T, (np.dot(data_x, theta) - data_y)) / n_samples + \\\n                   (2 * 0.01 / n_samples) * theta\n        theta -= alpha * gradient\n    return theta",
  "test_code": "import numpy as np\nfrom solution import run_linear_regression, sum_of_square_error, mean_absolute_error, run_steep_gradient_descent\n\n# Test data\ndata_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\ndata_y = np.array([4, 5, 6, 7])\ntheta = run_linear_regression(data_x, data_y)\n\ndef test_run_linear_regression():\n    expected_theta = np.array([[4.00244117, 0.00982216]])\n    np.testing.assert_almost_equal(theta, expected_theta, decimal=5)\n\ndef test_sum_of_square_error():\n    y_true = np.array([4, 5, 6, 7])\n    y_pred = np.dot(data_x, expected_theta.T)\n    assert sum_of_square_error(y_true, y_pred) == 0.03074165673623418\n\ndef test_mean_absolute_error():\n    y_true = np.array([4, 5, 6, 7])\n    y_pred = np.dot(data_x, expected_theta.T)\n    assert mean_absolute_error(y_true, y_pred) == 0.012724940181889844\n\ndef test_run_steep_gradient_descent():\n    theta_init = np.zeros((1, 2))\n    alpha = 0.000155\n    num_iterations = 100\n    theta = run_steep_gradient_descent(data_x, data_y, theta_init, alpha, num_iterations)\n    assert np.allclose(theta, expected_theta, atol=1e-5)",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results1.jsonl"
}