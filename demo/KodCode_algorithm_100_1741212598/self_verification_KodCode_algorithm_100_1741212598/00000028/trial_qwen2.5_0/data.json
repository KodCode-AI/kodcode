{
  "metadata": {
    "prompt_id": "00000028",
    "row_id": 28,
    "seed_ids": [
      112
    ],
    "mode": "algorithm"
  },
  "instruction": "### Question\n\nYou are working on a project to predict the `Rating` of players based on their `ADR` (Average Damage Per Round) in a multiplayer game. You have collected the data and want to use linear regression to model the relationship between `ADR` and `Rating`.\n\n**Task**:\nImplement the `run_linear_regression` function to perform linear regression on the provided dataset. Additionally, enhance the model by implementing `L2 regularization` to prevent overfitting.\n\n**Input**:\n- `data_x`: A matrix of shape `(n_samples, n_features + 1)` where each row represents a sample and the first column is a bias term.\n- `data_y`: A vector of shape `(n_samples,)` containing the corresponding target values.\n\n**Output**:\n- `theta`: A vector of shape `(1, n_features + 1)` representing the feature vector of the line of best fit.\n\n**Constraints**:\n- The learning rate `alpha` should be `0.0001550`.\n- The number of iterations should be `100000`.\n- Regularization parameter `lambda` should be `0.01`.\n\n**Performance Requirement**:\n- The implementation should be efficient and avoid overfitting.\n\n**Helper Functions**:\n- `run_steep_gradient_descent`: Performs a single step of gradient descent.\n- `sum_of_square_error`: Computes the sum of square error.\n- `mean_absolute_error`: Computes the mean absolute error.\n\n**Note**:\n- Ensure that your implementation is self-contained and does not reference the provided code snippet directly.\n- Test your implementation using the provided dataset.\n\n### Example\n\n```python\ndata_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\ndata_y = np.array([4, 5, 6, 7])\ntheta = run_linear_regression(data_x, data_y)\nprint(theta)  # Should print approximately [4.0, 0.01]\n```\n\n### Scoring\n\n- **Correct Implementation**: 70 points.\n- **Efficient Implementation (Regularization)**: 30 points.",
  "solution_code": "import numpy as np\n\ndef run_steep_gradient_descent(x, y, theta, alpha, lambda_reg, iterations):\n    n = x.shape[1]\n    m = len(y)\n    for _ in range(iterations):\n        error = x.dot(theta) - y\n        gradient = x.T.dot(error) / m + 2 * lambda_reg * theta\n        theta -= alpha * gradient\n    return theta\n\ndef sum_of_square_error(y_true, y_pred):\n    return np.sum((y_true - y_pred) ** 2)\n\ndef mean_absolute_error(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))\n\ndef run_linear_regression(data_x, data_y):\n    alpha = 0.0001550\n    lambda_reg = 0.01\n    iterations = 100000\n    theta = np.zeros((1, data_x.shape[1]))\n    \n    theta = run_steep_gradient_descent(data_x, data_y, theta, alpha, lambda_reg, iterations)\n    \n    return theta",
  "test_code": "import numpy as np\nfrom solution import run_linear_regression, run_steep_gradient_descent, sum_of_square_error, mean_absolute_error\n\ndef test_linear_regression():\n    data_x = np.array([[1, 50], [1, 70], [1, 90], [1, 110]])\n    data_y = np.array([4, 5, 6, 7])\n    theta = run_linear_regression(data_x, data_y)\n    assert theta.shape == (1, 2)\n    y_pred = data_x.dot(theta.T)\n    assert np.isclose(sum_of_square_error(data_y, y_pred), 0.0644, atol=1e-4)\n    assert np.isclose(mean_absolute_error(data_y, y_pred), 0.0161, atol=1e-4)\n\ndef test_steepest_gradient_descent():\n    np.random.seed(0)\n    X = np.random.rand(100, 2)\n    X = np.c_[np.ones((X.shape[0], 1)), X]  # adding bias term\n    y = 2 + 3 * X[:, 1] + 4 * X[:, 2] + np.random.randn(100) * 0.1  # y = 2 + 3x1 + 4x2\n    theta = np.zeros((1, 3))\n    alpha = 0.0001550\n    lambda_reg = 0.01\n    iterations = 100000\n    theta = run_steep_gradient_descent(X, y, theta, alpha, lambda_reg, iterations)\n    y_pred = X.dot(theta.T)\n    assert np.isclose(sum_of_square_error(y, y_pred), 0.0753, atol=1e-3)\n    assert np.isclose(mean_absolute_error(y, y_pred), 0.0190, atol=1e-3)\n\n# Running tests\ntest_linear_regression()\ntest_steepest_gradient_descent()",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results0.jsonl"
}