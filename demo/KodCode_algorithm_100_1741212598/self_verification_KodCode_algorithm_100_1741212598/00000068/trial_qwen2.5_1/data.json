{
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm"
  },
  "instruction": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution_code": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Applies a specified activation function to the input vector based on the given mode.\n    \"\"\"\n    if mode == 'binary_step':\n        return (vector > 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unsupported mode. Choose from 'binary_step', 'relu', 'sigmoid'.\")",
  "test_code": "import numpy as np\nfrom solution import activation_function\n\ndef test_binary_step():\n    assert np.array_equal(activation_function(np.array([-1.2, 0, 2, 1.45, -3.7, 0.3]), 'binary_step'), np.array([0, 1, 1, 1, 0, 1]))\n\ndef test_relu():\n    assert np.array_equal(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'), np.array([0, 0, 1, 0, 2]))\n\ndef test_sigmoid():\n    vector = np.array([-1, 0, 1])\n    expected_output = np.array([0.26894142, 0.5, 0.73105858])\n    np.testing.assert_almost_equal(activation_function(vector, 'sigmoid'), expected_output)\n\ndef test_invalid_mode():\n    with np.testing.assert_raises(ValueError):\n        activation_function(np.array([-1, 2]), 'tanh')",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results1.jsonl"
}