{
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm"
  },
  "instruction": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution_code": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Applies a specific activation function to the input vector based on the mode.\n    \"\"\"\n    if mode == 'binary_step':\n        return (vector > 0).astype(int)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))\n\n# Example usage\nif __name__ == \"__main__\":\n    vector1 = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    vector2 = np.array([-1, 0, 1, -2, 2])\n    vector3 = np.array([-1, 0, 1])\n    \n    print(activation_function(vector1, 'binary_step'))\n    print(activation_function(vector2, 'relu'))\n    print(activation_function(vector3, 'sigmoid'))",
  "test_code": "import numpy as np\nfrom solution import activation_function\n\ndef test_activation_function():\n    # Binary Step\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 0, 1, 1, 0, 1])\n    assert np.array_equal(activation_function(vector, 'binary_step'), expected)\n\n    # ReLU\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    assert np.array_equal(activation_function(vector, 'relu'), expected)\n\n    # Sigmoid\n    vector = np.array([-1, 0, 1])\n    expected = np.round([0.26894142, 0.5, 0.73105858], decimals=9)\n    assert np.allclose(activation_function(vector, 'sigmoid'), expected)\n\ndef test_invalid_mode():\n    vector = np.array([-1, 0, 1])\n    with pytest.raises(ValueError):\n        activation_function(vector, 'invalid_mode')\n\n# Running tests\nif __name__ == \"__main__\":\n    test_activation_function()\n    print(\"All tests passed!\")",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results0.jsonl"
}