{
  "metadata": {
    "prompt_id": "00000068",
    "row_id": 68,
    "seed_ids": [
      910
    ],
    "mode": "algorithm"
  },
  "instruction": "**Scenario**: You are working on a machine learning project that requires an activation function for a neural network layer. The Binary Step function is a simple yet effective choice for your needs. However, you want to extend its functionality to handle multiple activation functions for different layers.\n\n**Task**: Implement a more generalized version of the activation function that can switch between different activation functions based on a given input vector. Specifically, your task is to implement a function `activation_function` that takes an input vector and a mode as input and applies the corresponding activation function.\n\n**Function Signature**: `def activation_function(vector: np.ndarray, mode: str) -> np.ndarray`\n\n- **Input**:\n  - `vector` (ndarray): A numpy array of numeric values.\n  - `mode` (str): A string indicating the activation mode to be used. The possible values are:\n    - `'binary_step'`: Applies the binary step function.\n    - `'relu'`: Applies the Rectified Linear Unit (ReLU) activation function, which returns 0 for negative values and the input value for non-negative values.\n    - `'sigmoid'`: Applies the sigmoid activation function, which maps the input to a range between 0 and 1.\n\n- **Output**:\n  - A numpy array `vector` after applying the specified activation function.\n\n- **Constraints**:\n  - The input vector will always be a 1D numpy array.\n  - The mode will always be one of the specified strings.\n\n- **Performance Requirement**:\n  - The function should be optimized for performance, especially for large input vectors.\n\n**Examples**:\n```python\nimport numpy as np\n\n# Binary Step Function\nvector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\nprint(activation_function(vector, 'binary_step'))\n# Output: array([0, 1, 1, 1, 0, 1])\n\n# ReLU Function\nprint(activation_function(np.array([-1, 0, 1, -2, 2]), 'relu'))\n# Output: array([0, 0, 1, 0, 2])\n\n# Sigmoid Function\nprint(activation_function(np.array([-1, 0, 1]), 'sigmoid'))\n# Output: array([0.26894142, 0.5         , 0.73105858])\n```",
  "solution_code": "import numpy as np\n\ndef activation_function(vector: np.ndarray, mode: str) -> np.ndarray:\n    \"\"\"\n    Returns the numpy array of the values after applying the specified activation function.\n    \"\"\"\n    if mode == 'binary_step':\n        return np.where(vector >= 0, 1, 0)\n    elif mode == 'relu':\n        return np.maximum(0, vector)\n    elif mode == 'sigmoid':\n        return 1 / (1 + np.exp(-vector))\n    else:\n        raise ValueError(\"Unknown activation mode: {}\".format(mode))",
  "test_code": "import numpy as np\nfrom scipy.special import expit  # Import the expit function for sigmoid\n\ndef test_activation_function():\n    # Binary Step Function\n    vector = np.array([-1.2, 0, 2, 1.45, -3.7, 0.3])\n    expected = np.array([0, 1, 1, 1, 0, 1])\n    result = activation_function(vector, 'binary_step')\n    assert np.array_equal(result, expected)\n\n    # ReLU Function\n    vector = np.array([-1, 0, 1, -2, 2])\n    expected = np.array([0, 0, 1, 0, 2])\n    result = activation_function(vector, 'relu')\n    assert np.array_equal(result, expected)\n\n    # Sigmoid Function\n    vector = np.array([-1, 0, 1])\n    expected = expit(vector)  # Using scipy's expit for exact sigmoid implementation\n    result = activation_function(vector, 'sigmoid')\n    assert np.allclose(result, expected)\n\n    # Unknown activation mode\n    try:\n        activation_function(np.array([-1, 0, 1]), 'unknown')\n    except ValueError:\n        pass  # Test passes if it raises the expected ValueError\n\ntest_activation_function()",
  "file_source": "KodCode_questions2sv_algorithm_100_1741212598_sanitized_prepared_results2.jsonl"
}