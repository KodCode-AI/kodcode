import torch
import os
import sys
import argparse
import json
import time
import random
import numpy as np
from tqdm import tqdm
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer, AutoModelForCausalLM
from jinja2 import Environment, FileSystemLoader, exceptions

################
# Configurations
################
def get_args():
    # Experiment Settings
    parser = argparse.ArgumentParser(description="Seed Question Generation Manager.")
    # Generation Parameters
    parser.add_argument("--total_prompts", type=int, default=1000, help="Total number of prompts to generate. If specified, repeat will be ignored.")
    parser.add_argument('--mode', default="leetcode", type=str, choices=["leetcode", "algorithm", "data_structure", "package", "apps", "codeforces", "code_contests", "taco", "docs", "evol", "prefill"])

    # System Settings
    parser.add_argument("--output_folder", type=str, default="../demo")
    parser.add_argument("--job_name", type=str, default=None, help="Job Name.")
    parser.add_argument("--timestamp", type=int, default=int(time.time()), help="Timestamp for the job. Also used as the random seed.")
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")

    # Below are for Mode: Prefil
    # For details of how this model works, please refer to: https://magpie-align.github.io/
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-Coder-7B-Instruct",
                        help="We will support more models in the future.")
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--top_p", type=float, default=1.0)
    parser.add_argument("--n", type=int, default=200, help="Number of samples to generate for one time.")
    parser.add_argument("--max_tokens", type=int, default=2048)
    parser.add_argument("--max_model_len", type=int, default=4096)

    parser.add_argument("--prefill_contents", type=str, default="Write a python function to", help="The contents to be prefilled in the prompt.")
    parser.add_argument("--disable_early_stopping", action="store_false", dest="early_stopping", help="Disable early stopping.")
    parser.add_argument("--shuffle", type=bool, default=True, help="Shuffle the outputs generated by vllm.")
    parser.add_argument("--skip_special_tokens", type=bool, default=True)

    parser.add_argument('--engine', default="vllm", type=str, choices=["vllm"])
    parser.add_argument("--device", type=str, default="0")
    parser.add_argument("--dtype", type=str, default="bfloat16", choices=["float16", "bfloat16"])
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="Number of GPUs to use for tensor parallelism.")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.95)
    parser.add_argument("--swap_space", type=float, default=2.0)

    return parser.parse_args()

def get_seed_prompt(input, mode):
    # Load prompt template from markdown file
    env = Environment(loader=FileSystemLoader('prompts'))
    
    if mode in ["leetcode", "codeforces"]:
        template = env.get_template('genq_from_seed.md').render()
        # Add sample questions
        for i, problem in enumerate(input, 1):
            template += f"[Question {i}]: {problem['description']}\n\n"
        
        template += f"[Question {len(input) + 1}]:"
        
    elif mode in ["algorithm", "data_structure"]:
        template = env.get_template('genq_from_snippets.md').render()
        # Add code snippets section
        template += "\n```python\n"
        template += input[0]['codes'] + "\n"
        template += "```"
    
    elif mode == "docs":
        template = env.get_template('genq_from_docs.md').render()
        # Add code snippets section
        template = template.replace("{PACKAGE_NAME}", input[0]['source_name'])
        template = template.replace("{CONTENT}", input[0]['content'])

    elif mode in ["package", "apps", "taco", "code_contests", "evol"]:
        template = env.get_template('genq_from_seed.md').render()
        # Add sample questions
        for i, problem in enumerate(input, 1):
            template += f"[Question {i}]: {problem['instruction']}\n\n"
        
        template += f"[Question {len(input) + 1}]:"
    
    return template

args = get_args()
print(f"Seed Question Generation Manager.\nArguments:\n{args}") # For logging

#################
# System Settings
#################

# Set random seed
if args.seed is not None:
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)

# Create output file / folder
if args.mode != "prefill":
    output_filename = f"KodCode_seeds2questions_{args.mode}_{args.total_prompts}_{args.timestamp}.jsonl"
else:
    output_filename = f"KodCode_prefill_{args.total_prompts}_{args.timestamp}_results.jsonl"
output_foldername = f"KodCode_{args.mode}_{args.total_prompts}_{args.timestamp}"
if not args.job_name:
    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)
    if not os.path.exists(f"{args.output_folder}/{output_foldername}"):
        os.makedirs(f"{args.output_folder}/{output_foldername}")
    output_dir = f"{args.output_folder}/{output_foldername}/{output_filename}"
else:
    output_dir = f"{args.output_folder}/{args.job_name}/{output_filename}"


#################
# Load Seeds
#################
if args.mode == "leetcode":
    with open('../seeds/leetcode/leetcode.json', 'r') as f:
        seed_problems = json.load(f)
elif args.mode == "codeforces":
    with open('../seeds/codeforces/codeforces.jsonl', 'r') as f:
        seed_problems = [json.loads(line) for line in f]
        for i, problem in enumerate(seed_problems):
            problem['id'] = i
            problem['instruction'] = problem['problem_statement']
elif args.mode == "taco":
    with open('../seeds/taco/taco_train_lite.json', 'r') as f:
        seed_problems = json.load(f)
        for i, problem in enumerate(seed_problems):
            problem['id'] = i
            problem['instruction'] = problem['question']
elif args.mode == "code_contests":
    with open('../seeds/code_contests/code_contests_train_lite.json', 'r') as f:
        seed_problems = json.load(f)
        for i, problem in enumerate(seed_problems):
            problem['id'] = i
            problem['instruction'] = problem['question']
elif args.mode == "apps":
    with open('../seeds/apps/apps_train_lite.json', 'r') as f:
        seed_problems = json.load(f)
        for i, problem in enumerate(seed_problems):
            problem['id'] = problem['problem_id']
            problem['instruction'] = problem['question']
elif args.mode == "algorithm":
    with open('../seeds/algorithms/algorithm-python.json', 'r') as f:
        seed_problems = json.load(f)
elif args.mode == "data_structure":
    with open('../seeds/data_structure/data-structure-python.json', 'r') as f:
        seed_problems = json.load(f)
elif args.mode == "docs":
    with open('../seeds/docs/kodcode-docs.json', 'r') as f:
        seed_problems = json.load(f)
elif args.mode == "package":
    with open('../seeds/package/package_instruct_train_2000.json', 'r') as f:
        seed_problems = json.load(f)
        # add id to each problem
        for i, problem in enumerate(seed_problems):
            problem['id'] = i
elif args.mode == "evol":
    with open('../seeds/evol/evol_instruct_train_2000.json', 'r') as f:
        seed_problems = json.load(f)
        # add id to each problem
        for i, problem in enumerate(seed_problems):
            problem['id'] = i

if args.mode != "prefill":
    print(f"Number of seed files: {len(seed_problems)}")


################
# Generate outputs
################
if args.mode != "prefill":
    results = []
    for i in tqdm(range(args.total_prompts)):
        # Select 3 random problems (since it is too short)
        if args.mode in ["leetcode"]:
            selected_problems = random.sample(seed_problems, 3)
            seed_prompt = get_seed_prompt(selected_problems, args.mode)
        # For other modes, we only need 1 problem
        else:
            selected_problems = random.sample(seed_problems, 1)
            seed_prompt = get_seed_prompt(selected_problems, args.mode)

        # Save outputs      
        result = {
            "messages": [
                {
                    "role": "user",
                    "content": seed_prompt
                }
            ],
            "metadata": {
                "prompt_id": f"{i:08d}",
                "row_id": i,
                "seed_ids": [problem['id'] for problem in selected_problems],
                "mode": args.mode,
            },
        }
        results.append(result)

    # Save the final results
    with open(output_dir, "w") as f:
        for result in results:
            f.write(json.dumps(result) + "\n")

else: 
    # Set the device
    os.environ["CUDA_VISIBLE_DEVICES"] = args.device
    # Set repeat
    args.repeat = int(np.ceil(args.total_prompts / args.n))
    # Create vllm instance  
    llm = LLM(model=args.model_path, 
            dtype=args.dtype,
            trust_remote_code=True,
            gpu_memory_utilization=args.gpu_memory_utilization,
            max_model_len=args.max_model_len,
            swap_space=args.swap_space,
            tensor_parallel_size=args.tensor_parallel_size,
            seed=args.seed if args.seed is not None else args.timestamp,
            enable_prefix_caching=True)
    
    # Obtain configs
    with open("model_configs.json", "r", encoding="utf-8") as f:
        model_configs = json.load(f)
        model_config = model_configs[args.model_path]
        pre_query_template = model_config["pre_query_template"]
        pre_query_template += args.prefill_contents

        stop_tokens = model_config["stop_tokens"]
        stop_tokens_assistant = model_config["stop_tokens_assistant"]
        stop_tokens += stop_tokens_assistant
        stop_token_ids = model_config["stop_token_ids"]
    
        # Process early stopping. We found that sometimes LLM will generate responses immediately after the \n token.
        if args.early_stopping:
            stop_tokens.append("\n")
    
        print(f"Pre-query template: {pre_query_template}")
        print(f"Stop tokens: {stop_tokens}")
        print(f"Stop token ids: {stop_token_ids}")
        
    # Define sampling parameters
    sampling_params = SamplingParams(
        n=args.n,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
        skip_special_tokens=args.skip_special_tokens,
        stop=stop_tokens,
        stop_token_ids=stop_token_ids,
    )
    
    results = []
    i = 0
    for rounds in tqdm(range(args.repeat)):
        # Generate outputs
        if args.engine == "vllm":
            output = llm.generate(pre_query_template, sampling_params)
            output_list = output[0].outputs
            if args.shuffle:
                random.shuffle(output_list)
        
        # Save outputs
        for completion in output_list:
            instruction = completion.text.strip()
            # Save outputs      
            result = {
                "messages": [
                    {
                        "role": "user",
                        "content": pre_query_template
                    },
                    {
                        "role": "assistant",
                        "content": args.prefill_contents + " " + instruction
                    }
                ],
                "metadata": {
                    "prompt_id": f"{i:08d}",
                    "row_id": i,
                    "seed_ids": [0],
                    "mode": args.mode,
                },
            }
            results.append(result)
            i += 1

    # Save the final results
    with open(output_dir, "w") as f:
        for result in results:
            f.write(json.dumps(result) + "\n")

print(f"Finished. Total prompts: {len(results)}")
print(f"Output directory: {output_dir}")

